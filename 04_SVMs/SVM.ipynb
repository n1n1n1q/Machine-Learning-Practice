{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- an ML algorithm, capable of performing linear / non-linear classification, regressions, novelty detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear SVM predicts a class by computing the decision function\n",
    "$$\\mathbf{\\theta}^T\\mathbf{x} = \\theta_0 x_0 + \\dots + \\theta_n x_n$$\n",
    "If the result is positive, then the class is positive. And vice-versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of main SVM concepts;  \n",
    "Apart from trying to fit the data to classify it, the SVM looks for most \"balanced\" line (if considering linear SVMs).  \n",
    "* Hard Margin: trying to fit the margin the \"stupid\" way. Very sensible to outliers.  \n",
    "* Soft Margin: in fact, lets some of the sample instances breaking the margin. The regularization term with should be considered. Depending on the value of the hyperparameter, margin-breaking conditions become more or less stricter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The margin linear SVM classifier objective can be expressed as the constrained optimization problem.\n",
    "> ##### Hard margin linear SVM classifier objective\n",
    "> minimize ${1\\over 2} \\mathbf{w^\\intercal w}$  \n",
    "> subject to $t^{(i)}(\\mathbf{w^\\intercal x^{(i)}} + b)\\ge 1$, for $i = 1, 2, \\dots, m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a soft margin, the slack variable $\\zeta^{(i)} \\ge 0$ for each instance should be considered.  \n",
    "It measures how much the instance $i$ is allowed to violate the margin.\n",
    "\n",
    "> ##### Soft margin linear SVM classifier objective\n",
    "> minimize ${1\\over 2} \\mathbf{w^\\intercal w} + C \\sum_{i = 1}^m \\zeta^{(i)}$  \n",
    "> subject to $t^{(i)}(\\mathbf{w^\\intercal x^{(i)}} + b)\\ge 1 - \\zeta^{(i)}$ and $\\zeta^{(i)} \\ge 0 $, for $i = 1, 2, \\dots, m$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [ True False], decision_function: [ 0.66163816 -0.22035761]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "iris = load_iris(as_frame=True)\n",
    "X = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
    "y = (iris.target == 2)\n",
    "svm_clf = make_pipeline(StandardScaler(),\n",
    " LinearSVC(C=1, random_state=42))\n",
    "svm_clf.fit(X, y)\n",
    "X_new = [[5.5, 1.7], [5.0, 1.5]]\n",
    "print(f\"Predictions: {svm_clf.predict(X_new)}, decision_function: {svm_clf.decision_function(X_new)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, the dataset is not linearly seperable. In that case, adding more polynomial features can be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way is adding similarity features which measure how much each instance resembles a particular landmarl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More advanced approach is using kernel trick which allows to consider more features without adding complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of finding the largest margin, SVM tries to fit as many instances as possible without violating the margin rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dual Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a constrained optimization problem, known as the primal problem, it is possible to express a different but closely related problem, called its dual problem. The solution to the dual problem typically gives a lower bound to the solution of the primal problem, but under some conditions it can have the same solution as the  primal problem.  \n",
    "*Fortunately, the SVM problem meets these conditions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ##### Dual form of the linear SVM objective\n",
    "> $$\\text{minimize}_{\\alpha} \\quad \\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^m \\alpha^{(i)} \\alpha^{(j)} t^{(i)} t {(j)} \\mathbf{x}^{(i)} \\cdot \\mathbf{x}^{(j)} - \\sum_{i=1}^m \\alpha^{(i)} $$\n",
    "> $$ \\text{subject to} \\quad \\alpha^{(i)} \\geq 0 \\quad \\text{for all } i = 1, 2, \\dots, m, \\quad \\text{and} \\quad \\sum_{i=1}^m \\alpha^{(i)} t^{(i)} = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the vector $\\hat{\\alpha}$ is found, $\\hat{\\mathbf{w}}$ and $\\hat{b}$ can be computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ##### From the dual solution to the primal solution\n",
    ">$$\\mathbf{w} = \\sum_{i=1}^m \\hat{\\alpha}^{(i)} t^{(i)} \\mathbf{x}^{(i)}$$\n",
    ">$$\\hat{b} = \\frac{1}{n_s} \\sum_{i=1}^m \\left( t^{(i)} - \\mathbf{w} \\cdot \\mathbf{x}^{(i)} \\right) \\quad \\text{where} \\quad \\hat{\\alpha}^{(i)} > 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernelized SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you want to apply a second-degree polynomial transformation to a twodimensional training set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #####  Second-degree polynomial mapping\n",
    "> $$\\varphi(\\mathbf{x}) = \\varphi \\left( \n",
    "\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\right) = \n",
    "\\begin{bmatrix} \n",
    "x_1^2 \\\\ \n",
    "\\sqrt{2} x_1 x_2 \\\\ \n",
    "x_2^2 \n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, considering the dual problem, the following can be achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ##### Kernel trick for a second-degree polynomial mapping\n",
    "> $$\\varphi(\\mathbf{a})^\\top \\varphi(\\mathbf{b}) = \n",
    "\\begin{bmatrix}\n",
    "a_1^2 \\\\\n",
    "\\sqrt{2} a_1 a_2 \\\\\n",
    "a_2^2\n",
    "\\end{bmatrix}^\\top\n",
    "\\begin{bmatrix}\n",
    "b_1^2 \\\\\n",
    "\\sqrt{2} b_1 b_2 \\\\\n",
    "b_2^2\n",
    "\\end{bmatrix} \n",
    "= a_1^2 b_1^2 + 2 a_1 b_1 a_2 b_2 + a_2^2 b_2^2 $$\n",
    "> $$\n",
    "= (a_1 b_1 + a_2 b_2)^2 = \n",
    "\\left( \\begin{bmatrix} a_1 \\\\ a_2 \\end{bmatrix}^\\top \n",
    "\\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix} \\right)^2 \n",
    "= (\\mathbf{a}^\\top \\mathbf{b})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result will be the same, but the second option is way less computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, **Kernel** is a function capable of computing the dot product $\\varphi(\\mathbf{a})^\\intercal \\varphi(\\mathbf{b})$, based only on the original vectors a and b, without having to compute the transformation $\\varphi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ##### Common kernels\n",
    "> Linear:     $K(\\mathbf{a}, \\mathbf{b}) = \\mathbf{a}^\\top \\mathbf{b} $  \n",
    "> Polynomial:   $K(\\mathbf{a}, \\mathbf{b}) = (\\gamma \\mathbf{a}^\\top \\mathbf{b} + r)^d$  \n",
    "> Gaussian RBF:  $K(\\mathbf{a}, \\mathbf{b}) = \\exp\\left(-\\gamma \\|\\mathbf{a} - \\mathbf{b}\\|^2\\right)$  \n",
    "> Sigmoid:    $K(\\mathbf{a}, \\mathbf{b}) = \\tanh\\left(\\gamma \\mathbf{a}^\\top \\mathbf{b} + r\\right)$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ##### Making predictions with a kernelized SVM\n",
    "> $$\n",
    "h_{\\mathbf{w}, \\hat{b}}(\\varphi(\\mathbf{x}^{(n)})) = \n",
    "\\mathbf{\\hat{w}}^\\top \\varphi(\\mathbf{x}^{(n)}) + \\hat{b}\n",
    "= \\left( \\sum_{i=1}^m \\hat{\\alpha}^{(i)} t^{(i)} \\varphi(\\mathbf{x}^{(i)}) \\right)^\\top \\varphi(\\mathbf{x}^{(n)}) + \\hat{b} $$\n",
    "> $$\n",
    "= \\sum_{i=1}^m \\hat{\\alpha}^{(i)} t^{(i)} \\left( \\varphi(\\mathbf{x}^{(i)})^\\top \\varphi(\\mathbf{x}^{(n)}) \\right) + \\hat{b}$$\n",
    "> $$\n",
    "= \\sum_{i=1}^m \\hat{\\alpha}^{(i)} t^{(i)} K(\\mathbf{x}^{(i)}, \\mathbf{x}^{(n)}) + \\hat{b}\n",
    "\\quad \\text{where} \\quad \\hat{\\alpha}^{(i)} > 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ##### Using the kernel trick to compute the bias term\n",
    "> $$ \\hat{b} = \\frac{1}{n_s} \\sum_{i=1 \\atop \\hat{\\alpha}^{(i)} > 0}^m \\left(t^{(i)} - \\hat{\\mathbf{w}}^T\\varphi(\\mathbf{x}^{(i)})\\right) = \\frac{1}{n_s} \\sum_{i=1 \\atop \\hat{\\alpha}^{(i)} > 0}^m \\left(t^{(i)} - \\left(\\sum_{j=1}^m \\hat{\\alpha}^{(j)} t^{(j)} \\varphi(\\mathbf{x}^{(j)})\\right)^T \\varphi(\\mathbf{x}^{(i)})\\right) $$\n",
    "> $$= \\frac{1}{n_s} \\sum_{i=1 \\atop \\hat{\\alpha}^{(i)} > 0}^m \\left(t^{(i)} - \\sum_{j=1 \\atop \\hat{\\alpha}^{(j)} > 0}^m \\hat{\\alpha}^{(j)} t^{(j)} K(\\mathbf{x}^{(i)}, \\mathbf{x}^{(j)})\\right) $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
